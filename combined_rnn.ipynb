{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import (\n",
    "    train_tokens_tokenized, train_entities, train_intents,\n",
    "    test_tokens_tokenized, test_entities, test_intents,\n",
    "    dev_tokens_tokenized, dev_entities, dev_intents, current_int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Create a dataset class\n",
    "class PizzaDataset(Dataset):\n",
    "    def __init__(self, tokens, entities, intents):\n",
    "        self.tokens = tokens\n",
    "        self.entities = entities\n",
    "        self.intents = intents\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.tokens[idx]), torch.tensor(self.entities[idx]), torch.tensor(self.intents[idx])\n",
    "\n",
    "# Define the RNN model\n",
    "class CombinedRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, label_size, embedding_dim, hidden_dim):\n",
    "        super(CombinedRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_entity = nn.Linear(hidden_dim, label_size)\n",
    "        self.fc_intent = nn.Linear(hidden_dim, label_size)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.embedding(tokens)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        rnn_out, _ = self.rnn(embeddings)\n",
    "        entity_logits = self.fc_entity(rnn_out)  # Shape: (batch_size, seq_len, label_size)\n",
    "        intent_logits = self.fc_intent(rnn_out)  # Shape: (batch_size, seq_len, label_size)\n",
    "        return entity_logits, intent_logits\n",
    "\n",
    "# Padding function for the DataLoader\n",
    "def collate_fn(batch):\n",
    "    tokens, entities, intents = zip(*batch)\n",
    "\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    tokens_padded = pad_sequence(tokens, batch_first=True, padding_value=-1)\n",
    "    entities_padded = pad_sequence(entities, batch_first=True, padding_value=-1)\n",
    "    intents_padded = pad_sequence(intents, batch_first=True, padding_value=-1)\n",
    "\n",
    "    return tokens_padded, entities_padded, intents_padded\n",
    "\n",
    "# Create the DataLoader\n",
    "def data_loader(tokens,entities,intents):\n",
    "    dataset = PizzaDataset(tokens, entities, intents)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "train_dataloader = data_loader(train_tokens_tokenized, train_entities, train_intents)\n",
    "test_dataloader = data_loader(test_tokens_tokenized, test_entities, test_intents)\n",
    "dev_dataloader = data_loader(dev_tokens_tokenized, dev_entities, dev_intents)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore padding index (-1)\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "VOCAB_SIZE = current_int \n",
    "LABEL_SIZE = 23 # label.map length\n",
    "\n",
    "model = CombinedRNN(VOCAB_SIZE, LABEL_SIZE, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for tokens, entities, intents in dataloader:\n",
    "            tokens, entities, intents = tokens.to(device), entities.to(device), intents.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            entity_logits, intent_logits = model(tokens)\n",
    "\n",
    "            # Compute loss\n",
    "            loss_entities = criterion(entity_logits.view(-1, LABEL_SIZE), entities.view(-1))\n",
    "            loss_intents = criterion(intent_logits.view(-1, LABEL_SIZE), intents.view(-1))\n",
    "            loss = loss_entities + loss_intents\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_dataloader, criterion, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataloader,printValues=True):\n",
    "    model.eval()\n",
    "    correct_entities = 0\n",
    "    correct_intents = 0\n",
    "    total_entities = 0\n",
    "    total_intents = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tokens, entities, intents in dataloader:\n",
    "            tokens, entities, intents = tokens.to(device), entities.to(device), intents.to(device)\n",
    "            # Forward pass\n",
    "            entity_logits, intent_logits = model(tokens)\n",
    "\n",
    "            # Calculate predictions\n",
    "            _, predicted_entities = torch.max(entity_logits, dim=-1)\n",
    "            _, predicted_intents = torch.max(intent_logits, dim=-1)\n",
    "            if printValues:\n",
    "                # Print actual and predicted intents and entities\n",
    "                print(\"Predicted Intents:\", predicted_intents)\n",
    "                print(\"Actual Intents:   \", intents)\n",
    "                print(\"Predicted Entities:\", predicted_entities)\n",
    "                print(\"Actual Entities:   \", entities)\n",
    "\n",
    "            # Calculate accuracy for entities (ignoring padding)\n",
    "            mask = entities != -1  # Ignore padding\n",
    "            \n",
    "            # print(predicted_entities.shape,entities.shape)\n",
    "            correct_entities += (predicted_entities == entities).masked_select(mask).sum().item()\n",
    "            total_entities += mask.sum().item()\n",
    "\n",
    "            # Calculate accuracy for intents\n",
    "            mask = intents != -1\n",
    "            correct_intents += (predicted_intents == intents).masked_select(mask).sum().item()\n",
    "            total_intents += mask.sum().item()\n",
    "\n",
    "    # Compute final accuracies\n",
    "    entity_accuracy = correct_entities / total_entities if total_entities > 0 else 0\n",
    "    intent_accuracy = correct_intents / total_intents if total_intents > 0 else 0\n",
    "    print(f\"Entity Accuracy: {entity_accuracy:.4f}, Intent Accuracy: {intent_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_accuracy(model, test_dataloader, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_accuracy(model, dev_dataloader,False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
